{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07a2c86-23aa-4848-b413-2b351614b4b6",
   "metadata": {},
   "source": [
    "# NLP tf-idf pipeline with cuML + Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f8a3c-70da-4dd3-8e2e-655452dfa306",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait\n",
    "import dask_cudf\n",
    "from cuml.feature_extraction.text import HashingVectorizer\n",
    "from cuml.dask.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "import cupy as cp\n",
    "from tqdm import tqdm\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc367b02-8ab3-45b1-aa93-cc55bb2ed560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other utility functions for benhmarking purposes\n",
    "from utils import SimpleTimer, ResultsLogger, scale_workers\n",
    "from utils import visualize_data_cuml as visualize_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850d8cf-a343-476e-9c47-a0718e6e04bf",
   "metadata": {},
   "source": [
    "## Setting up the Dask cuda cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb9b87-94af-49f9-b00d-682305b7c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a local CUDA cluster\n",
    "cluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\", local_directory=\"/raid/anirband/dask\", rmm_pool_size=\"30GB\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b987af-61e4-4693-bfc1-5ba6cb96ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abd114-7d65-4081-8a04-580e4a5785d8",
   "metadata": {},
   "source": [
    "## Benchmarking Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697d77b-999d-4737-b9ff-f017dbba3868",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8f7cb-8837-4ac0-8e21-3ed06665c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "PUNCTUATIONS = ['!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.',\n",
    "                '/', '\\\\', ':', ';', '<', '=', '>', '?', '@', '[', ']', '^',\n",
    "                '_', '`', '{', '|', '}', '\\t','\\n', \"'\", \",\", '~' , 'â€”']\n",
    "\n",
    "def read_data(client, parquet_path, benchmark):\n",
    "    data = dask_cudf.read_parquet(parquet_path, columns=[\"review_body\"], row_groups_per_part=3)\n",
    "    if benchmark:\n",
    "        data = client.persist(data)\n",
    "        wait(data)\n",
    "        print(data.shape[0].compute())\n",
    "    return data\n",
    "\n",
    "\n",
    "def text_preprocessor(data, client, column_name, PUNCTUATIONS,\n",
    "                      STOPWORDS, benchmark):\n",
    "    data = data[data[column_name].notnull()]\n",
    "    data[column_name] = (data[column_name]\n",
    "                         .str.lower()\n",
    "                         .str.replace_tokens(\n",
    "                             PUNCTUATIONS, [\" \"]*len(PUNCTUATIONS))\n",
    "                         .str.replace_tokens(STOPWORDS, \"\")\n",
    "                         .str.normalize_spaces()\n",
    "                         .str.strip())\n",
    "    if benchmark:\n",
    "        data = client.persist(data)\n",
    "        wait(data)\n",
    "        print(data.shape[0].compute())\n",
    "    return data\n",
    "\n",
    "\n",
    "def hashing_vectorizer(data, client, column_name, benchmark):\n",
    "    vectorizer = HashingVectorizer(stop_words=None, preprocessor=None)\n",
    "    # Meta is an empty dataframe matches the dtypes and columns of the output\n",
    "    meta = dask.array.from_array(cp.sparse.csr_matrix(cp.zeros(1, dtype=cp.float32)))\n",
    "    hashing_vectorized = data[column_name].map_partitions(vectorizer.fit_transform, meta=meta).astype(cp.float32)\n",
    "    if benchmark:\n",
    "        hashing_vectorized = client.persist(hashing_vectorized)\n",
    "        wait(hashing_vectorized)\n",
    "        hashing_vectorized.compute_chunk_sizes()\n",
    "        print(hashing_vectorized.shape)\n",
    "    return hashing_vectorized\n",
    "\n",
    "\n",
    "def tfidf_transformer(data, client, benchmark):\n",
    "    multi_gpu_transformer = TfidfTransformer(client=client)\n",
    "    result = multi_gpu_transformer.fit_transform(data)\n",
    "    if benchmark:\n",
    "        result = client.persist(result)\n",
    "        wait(result)\n",
    "        result.compute_chunk_sizes()\n",
    "        print(result.shape)\n",
    "    return result\n",
    "\n",
    "def tfidf_transformer_POC(data, client, benchmark):\n",
    "    multi_gpu_transformer = TfidfTransformer()\n",
    "    result = multi_gpu_transformer.fit_transform(data)\n",
    "    if benchmark:\n",
    "        result = client.persist(result)\n",
    "        wait(result)\n",
    "        result.compute_chunk_sizes()\n",
    "        print(result.shape)\n",
    "    return result\n",
    "\n",
    "\n",
    "def execute_full_pipeline(n, i, client, parquet_path, worker_counts=[1],\n",
    "                            result_path=\"./results.csv\", benchmark=True):\n",
    "    sample_record = {\"overall\": 0, \"data_read\": 0, \"hashing_vectorizer\": 0,\n",
    "                    \"tfidf_transformer\": 0, \"data_preprocessing\": 0, \"nrows\": 0}\n",
    "    # client.restart()\n",
    "    with SimpleTimer() as timer:\n",
    "        data = read_data(client, parquet_path, benchmark)\n",
    "    sample_record[\"data_read\"] = timer.elapsed/1e9\n",
    "\n",
    "    with SimpleTimer() as timer:\n",
    "        data = text_preprocessor(data, client, \"review_body\", PUNCTUATIONS, STOPWORDS, benchmark)\n",
    "    sample_record[\"data_preprocessing\"] = timer.elapsed/1e9\n",
    "\n",
    "    with SimpleTimer() as timer:\n",
    "        hashing_vectorized = hashing_vectorizer(data, client, \"review_body\", benchmark)\n",
    "    sample_record[\"hashing_vectorizer\"] = timer.elapsed/1e9\n",
    "\n",
    "    with SimpleTimer() as timer:\n",
    "        result = tfidf_transformer(hashing_vectorized, client, benchmark=True)\n",
    "    sample_record[\"tfidf_transformer\"] = timer.elapsed/1e9\n",
    "\n",
    "    print(f\"Workers:{n}, Sample Run:{i}, Finished loading data in {sample_record['data_read']}s\")\n",
    "    print(f\"Workers:{n}, Sample Run:{i}, Finished preprocessing data in {sample_record['data_preprocessing']}s\")\n",
    "    print(f\"Workers:{n}, Sample Run:{i}, Finished fitting HashVectorizer in {sample_record['hashing_vectorizer']}s\")\n",
    "    print(f\"Workers:{n}, Sample Run:{i}, Finished fitting IDF Transformer in {sample_record['tfidf_transformer']}s\")\n",
    "\n",
    "    return data, result, sample_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b3081-f947-4f87-8443-315c53053612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_numbers(client, parquet_path, worker_counts=[1], samples=1, result_path=\"./results.csv\", benchmark=True):\n",
    "    \"\"\"\n",
    "    Main function to perform the performance sweep\n",
    "    \"\"\"\n",
    "    results_logger = ResultsLogger(result_path)\n",
    "    for n in worker_counts: \n",
    "        scale_workers(client, n)\n",
    "        \n",
    "        for i in tqdm(range(samples)): \n",
    "            with SimpleTimer() as overalltimer:\n",
    "                data, result, sample_record = execute_full_pipeline(n, i, client, parquet_path, worker_counts=[1], result_path=result_path, benchmark=benchmark)\n",
    "            sample_record[\"overall\"]=overalltimer.elapsed/1e9\n",
    "            sample_record[\"nrows\"]=data.shape[0].compute()\n",
    "            sample_record[\"n_workers\"]=n\n",
    "            sample_record[\"sample_run\"]=i\n",
    "            print(f\"Workers:{n}, Sample Run:{i}, Finished executing full pipeline in {overalltimer.elapsed/1e9}s\")\n",
    "            results_logger.log(sample_record)\n",
    "    results_logger.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd559c-e7b9-4386-b819-c9a54cd1e140",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark latency by materializing the intermediate dataframe(s) in every stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa939d6-3158-4577-b7ef-6fb4f8bec54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet_path = 's3://amazon-reviews-pds/parquet/product_category=Camera/*.parquet'\n",
    "dataset = \"Books\"\n",
    "#parquet_path = f'./data/product_category={dataset}'\n",
    "parquet_path = f\"/raid/amazon_reviews_dataset/product_category={dataset}\"\n",
    "samples = 5\n",
    "worker_counts = [8] # [2,4,6,8]\n",
    "result_path = f\"./results/result_poc_nlp_dask_{dataset}_persist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2c546-c633-4f77-9220-e0067add959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "performance_numbers(client, parquet_path=parquet_path, worker_counts=worker_counts, samples=samples, result_path=result_path, benchmark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd627f9-0878-4296-bf73-151ec1b1bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, melt_data = visualize_data(result_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc1a20-d436-4362-af91-75d4b6e829b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby = data.groupby(\"n_workers\").agg(['mean', 'std', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac4d3f-5d2f-4941-a984-adad8d15edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b1dab-3ff0-40f7-9889-b744704f8cec",
   "metadata": {},
   "source": [
    "## Benchmark latency without materializing the intermediate dataframe(s) except for the last stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21d548-c7c1-40d4-b054-32c3f1206fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart() # restart the client before performing the following set of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db8890-5f91-426c-8b72-bcf2c758f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet_path = 's3://amazon-reviews-pds/parquet/product_category=Camera/*.parquet'\n",
    "dataset = \"Books\"\n",
    "#parquet_path = f'./data/product_category={dataset}'\n",
    "parquet_path = f\"/raid/amazon_reviews_dataset/product_category={dataset}\"\n",
    "samples = 5\n",
    "worker_counts = [8] # [2,4,6,8]\n",
    "result_path = f\"./results/result_poc_nlp_dask_{dataset}_nonpersist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f8e22-e8e0-4fd8-8b64-d619b7c42b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "performance_numbers(client, parquet_path=parquet_path, worker_counts=worker_counts, samples=samples, result_path=result_path, benchmark=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ebbdb-002a-41c7-aa6e-4ad3c3fa45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, melt_data = visualize_data(result_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2bf60-f3d2-4027-9417-8dcc15249c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby = data.groupby(\"n_workers\").agg(['mean', 'std', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce3774-81dd-4029-b8d5-63a1707f006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec87629-1843-4b99-97d3-c33baea7dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34f385-6187-4753-adc3-cd9bc2fe90b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
